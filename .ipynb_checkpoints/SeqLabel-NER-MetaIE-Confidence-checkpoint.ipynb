{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f5f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json, jsonlines\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "path = f\"models/roberta-large-metaie-5-shot-conll2003\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "tagger = AutoModelForTokenClassification.from_pretrained(path).to(device)\n",
    "\n",
    "ent_names = ['Organization', 'Other entity', 'Person', 'Location']\n",
    "\n",
    "def find_sequences(lst):\n",
    "    sequences = []\n",
    "    i = 0\n",
    "    while i < len(lst):\n",
    "        if lst[i] == 0:\n",
    "            start = i\n",
    "            end = i\n",
    "            i += 1\n",
    "            while i < len(lst) and lst[i] == 1:\n",
    "                end = i\n",
    "                i += 1\n",
    "            sequences.append((start, end+1))\n",
    "        else:\n",
    "            i += 1\n",
    "    return sequences\n",
    "\n",
    "def is_sublst(lst1, lst2):\n",
    "    for idx in range(len(lst1)-len(lst2)+1):\n",
    "        if lst1[idx:idx+len(lst2)] == lst2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_tuples(data):\n",
    "    filtered_data = []\n",
    "\n",
    "    for current in data:\n",
    "        conflicts = [other for other in data if other[0] == current[0] and other[2] != current[2]]\n",
    "        \n",
    "        if not conflicts:\n",
    "            filtered_data.append(current)\n",
    "        else:\n",
    "            should_add = True\n",
    "            for conflict in conflicts:\n",
    "                if conflict[1] > current[1]:\n",
    "                    should_add = False\n",
    "                    break\n",
    "            \n",
    "            if should_add:\n",
    "                filtered_data.append(current)\n",
    "    \n",
    "    unique_filtered_data = list(set(filtered_data))\n",
    "\n",
    "    return unique_filtered_data\n",
    "\n",
    "T, P, TP = 0, 0, 0\n",
    "\n",
    "bar = tqdm([data for data in jsonlines.open(f\"dataset/test.conll2003.json\")])\n",
    "\n",
    "PRED, REF = [], []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for item in bar:\n",
    "    sentence = \" \".join(item[\"words\"])\n",
    "    label = \" \".join(item[\"words\"][:item[\"words\"].index(':')])\n",
    "\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "    logits = tagger(**inputs).logits[0, 1:-1]\n",
    "    confidences = logits.amax(-1)\n",
    "    tag_predictions = logits.argmax(-1)\n",
    "    tag_references = [[\"B\", \"I\", \"O\"].index(tag) for tag in item[\"ner\"]]\n",
    "    \n",
    "    predictions = [(tokenizer.decode(inputs.input_ids[0, 1:-1][seq[0]:seq[1]]).strip(), confidences[seq[0]:seq[1]].mean().item(), label) for seq in find_sequences(tag_predictions)]\n",
    "    predictions = [prediction for prediction in predictions if is_sublst(item[\"words\"], prediction[0].split())]\n",
    "    references = [(\" \".join(item[\"words\"][seq[0]:seq[1]]), label) for seq in find_sequences(tag_references)]\n",
    "    \n",
    "    PRED.extend(predictions)\n",
    "    REF.extend(references)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n",
    "    if cnt % len(ent_names) == 0:\n",
    "        PRED = filter_tuples(PRED)\n",
    "        PRED = [(tup[0], tup[2]) for tup in PRED]\n",
    "    \n",
    "        P += len(PRED)\n",
    "        T += len(REF)\n",
    "\n",
    "        TP += len([prediction for prediction in PRED if prediction in REF])\n",
    "\n",
    "        Prec = TP/(P+1e-8)\n",
    "        Rec = TP/(T+1e-8)\n",
    "        F1 = TP*2/(T+P+1e-8)\n",
    "\n",
    "        bar.set_description(f\"#Prec. = {Prec*100:.4} #Rec. = {Rec*100:.4} #F1 = {F1*100:.4}\")\n",
    "        \n",
    "        PRED, REF = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a8381d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "komeiji",
   "language": "python",
   "name": "komeiji"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
